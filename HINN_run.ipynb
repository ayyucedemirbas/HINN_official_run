{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyucedemirbas/HINN_official_run/blob/main/HINN_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uj6G99Rjnmco"
      },
      "outputs": [],
      "source": [
        "!pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXgAaVgWn1SO",
        "outputId": "a621209c-0fcc-456b-dca1-e2c9abacf9bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'HINN' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/bozdaglab/HINN.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNZOd7nzn2y0",
        "outputId": "76f230c5-a5db-46f4-90f9-951767891632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/HINN\n"
          ]
        }
      ],
      "source": [
        "%cd HINN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE2UKgg-oWF8",
        "outputId": "2c835212-4820-4120-9694-eef36a3f5a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "foJFMQcin6HX"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# SECTION: Library Imports\n",
        "# ---------------------------\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, BatchNormalization, Concatenate\n",
        "from keras import regularizers\n",
        "from keras.utils import plot_model\n",
        "\n",
        "# Set backend for Keras to use PyTorch\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "\n",
        "import captum\n",
        "from captum.attr import DeepLift\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import plotly.colors as pc\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# SECTION: Data Loading\n",
        "# ---------------------------\n",
        "\n",
        "def load_and_process_data():\n",
        "    def preprocess(file_path, suffix):\n",
        "        df = pd.read_csv(file_path)\n",
        "        df.index = df.iloc[:, 0]\n",
        "        df = df.drop(df.columns[0], axis=1)\n",
        "        df.columns = [f\"{col}_{suffix}\" for col in df.columns]\n",
        "        return df\n",
        "\n",
        "    expression = preprocess(\"gene_data.csv\", \"expression\")\n",
        "    methy = preprocess(\"methyl_data.csv\", \"methy\")\n",
        "    snp = preprocess(\"snp_data.csv\", \"snp\")\n",
        "    demograph = pd.read_csv(\"demo_label_data.csv\", usecols=range(7))\n",
        "    demograph.index = demograph.iloc[:, 0]\n",
        "    demograph = demograph.drop(demograph.columns[0], axis=1)\n",
        "    demograph.columns = [f\"{col}_demograph\" for col in demograph.columns]\n",
        "\n",
        "    label = pd.read_csv(\"demo_label_data.csv\", usecols=[0, 8])\n",
        "    label.index = label.iloc[:, 0]\n",
        "    label = label.drop(label.columns[0], axis=1)\n",
        "    label.columns = [f\"{col}_label\" for col in label.columns]\n",
        "\n",
        "    # --- Inner join all datasets on their indices ---\n",
        "    data = snp.join(expression, how=\"inner\") \\\n",
        "              .join(methy, how=\"inner\") \\\n",
        "              .join(demograph, how=\"inner\") \\\n",
        "              .join(label, how=\"inner\")\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6dDiZm9nSX42"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# SECTION: Custom Keras Layers\n",
        "# ---------------------------\n",
        "\n",
        "class PrimaryInputLayer(nn.Module):\n",
        "    def __init__(self, units, output_dim, activation=\"sigmoid\", mask=None):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # activation\n",
        "        if activation == \"sigmoid\":\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
        "\n",
        "        # trainable weight and bias\n",
        "        self.w = nn.Parameter(torch.empty(units, output_dim))\n",
        "        self.b = nn.Parameter(torch.zeros(output_dim))\n",
        "\n",
        "        nn.init.xavier_normal_(self.w)\n",
        "\n",
        "        # non-trainable mask (same shape as w)\n",
        "        if mask is None:\n",
        "            raise ValueError(\"mask tensor is required\")\n",
        "        self.register_buffer(\"mask\", mask.float())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, units)\n",
        "        masked_w = self.w * self.mask\n",
        "        out = x @ masked_w + self.b\n",
        "        return self.activation(out)\n",
        "\n",
        "\n",
        "class SecondaryInputLayer(nn.Module):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "\n",
        "        # diagonal mask\n",
        "        self.register_buffer(\"mask\", torch.eye(units))\n",
        "\n",
        "        self.w = nn.Parameter(torch.empty(units, units))\n",
        "        nn.init.xavier_normal_(self.w)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, units)\n",
        "        masked_w = self.w * self.mask\n",
        "        return x @ masked_w\n",
        "\n",
        "\n",
        "class MultiplicationInputLayer(nn.Module):\n",
        "    def __init__(self, units, activation=\"sigmoid\"):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "\n",
        "        if activation == \"sigmoid\":\n",
        "            self.activation = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
        "\n",
        "        self.b = nn.Parameter(torch.zeros(units))\n",
        "        nn.init.xavier_normal_(self.b.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(x + self.b)\n",
        "\n",
        "# ---------------------------\n",
        "# SECTION: Dataset for PyTorch\n",
        "# ---------------------------\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return [input[idx] for input in self.inputs], self.targets[idx]\n",
        "\n",
        "# ---------------------------\n",
        "# SECTION: Early Stopping\n",
        "# ---------------------------\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=50, delta=0.0, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.counter = 0\n",
        "        self.best_model_state = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if isinstance(val_loss, torch.Tensor):\n",
        "            val_loss = val_loss.item()\n",
        "\n",
        "        if val_loss < self.best_loss - self.delta:\n",
        "            # Improvement: reset counter and save best state\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            if self.restore_best_weights:\n",
        "                self.best_model_state = model.state_dict()\n",
        "        else:\n",
        "            # No improvement\n",
        "            self.counter += 1\n",
        "\n",
        "        # Check patience\n",
        "        if self.counter >= self.patience:\n",
        "            print(f\"⏹ Early stopping triggered. Best val_loss = {self.best_loss:.4f}\")\n",
        "            if self.restore_best_weights and self.best_model_state is not None:\n",
        "                model.load_state_dict(self.best_model_state)\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "# ---------------------------\n",
        "# SECTION: Model Training Pipeline\n",
        "# ---------------------------\n",
        "\n",
        "def train_model_torch(model, train_loader, val_loader, device=\"cpu\",\n",
        "                      lr=1e-3, epochs=1000, patience=500):\n",
        "    criterion = torch.nn.L1Loss()  # MAE\n",
        "    # criterion = torch.nn.MSELoss()  # MSE\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    early_stopper = EarlyStopping(patience=patience, delta=0.0, restore_best_weights=True)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ----- TRAIN -----\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = [x.to(device).float() for x in inputs]\n",
        "            targets = targets.to(device).float()\n",
        "\n",
        "            # Handle batch size of 1 during training\n",
        "            if inputs[0].size(0) == 1:\n",
        "                model.eval()  # Switch to eval mode for batch size 1\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(*inputs).squeeze()\n",
        "                model.train() # Switch back to train mode\n",
        "            else:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(*inputs).squeeze()\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item() * targets.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # ----- VAL -----\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs = [x.to(device).float() for x in inputs]\n",
        "                targets = targets.to(device).float()\n",
        "                outputs = model(*inputs).squeeze()\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item() * targets.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
        "\n",
        "        # ----- EARLY STOPPING -----\n",
        "        if early_stopper(val_loss, model):\n",
        "            print(f\"Stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    return model\n",
        "\n",
        "# ---------------------------\n",
        "# SECTION: Evaluation Function\n",
        "# ---------------------------\n",
        "\n",
        "def evaluate_model_torch(model, test_loader, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    all_targets = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs = [x.to(device).float() for x in inputs]\n",
        "            targets = targets.to(device).float().unsqueeze(1)\n",
        "\n",
        "            # Handle batch size of 1 during evaluation as well\n",
        "            if inputs[0].size(0) == 1:\n",
        "                preds = model(*inputs)\n",
        "            else:\n",
        "                preds = model(*inputs)\n",
        "\n",
        "            all_targets.append(targets.cpu().numpy())\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "\n",
        "\n",
        "    y_true = np.concatenate(all_targets, axis=0).squeeze()\n",
        "    y_pred = np.concatenate(all_preds, axis=0).squeeze()\n",
        "\n",
        "    mse = np.mean((y_true - y_pred) ** 2)\n",
        "    mae = np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "    return {\"mse\": mse, \"mae\": mae, \"y_true\": y_true, \"y_pred\": y_pred}\n",
        "\n",
        " #---------------------------\n",
        "# SECTION: Interpretation\n",
        "# ---------------------------\n",
        "\n",
        "def interpret_model(model, test_inputs, baselines, device=\"cpu\"):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    test_inputs = tuple(t.to(device) for t in test_inputs)\n",
        "    baselines = tuple(b.to(device) for b in baselines)\n",
        "\n",
        "    explainer = DeepLift(model)\n",
        "    attributions = explainer.attribute(\n",
        "        test_inputs,\n",
        "        baselines=baselines,\n",
        "        return_convergence_delta=False,\n",
        "    )\n",
        "    return attributions\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# SECTION: Attribution Export\n",
        "# ---------------------------\n",
        "\n",
        "def export_attributions(attributions, feature_names, save_path_prefix):\n",
        "    for i, name in enumerate(['snp', 'methy', 'gene', 'demo']):\n",
        "        df = pd.DataFrame(attributions[i].detach().numpy(), columns=feature_names[i])\n",
        "        df.to_csv(f\"{save_path_prefix}_{name}.csv\", index=False)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# SECTION: Matrix Filtering and Sankey Plotting\n",
        "# ---------------------------\n",
        "\n",
        "def filter_matrices_by_top_features(snp_list, methy_list, gene_list,\n",
        "                                     sparse_methy, sparse_gene, sparse_pathway):\n",
        "    subset_methy_matrix = sparse_methy.loc[snp_list, methy_list]\n",
        "    subset_gene_matrix = sparse_gene.loc[methy_list, gene_list]\n",
        "    subset_pathway_matrix = sparse_pathway.loc[gene_list, :]\n",
        "\n",
        "    subset_methy_matrix = subset_methy_matrix.loc[subset_methy_matrix.any(axis=1) == 1, subset_methy_matrix.any(axis=0)]\n",
        "    subset_gene_matrix = subset_gene_matrix.loc[subset_gene_matrix.any(axis=1) == 1, subset_gene_matrix.any(axis=0)]\n",
        "    subset_pathway_matrix = subset_pathway_matrix.loc[subset_pathway_matrix.index.isin(subset_gene_matrix.columns)]\n",
        "    subset_pathway_matrix = subset_pathway_matrix.loc[subset_pathway_matrix.any(axis=1) == 1, subset_pathway_matrix.any(axis=0)]\n",
        "\n",
        "    return subset_methy_matrix, subset_gene_matrix, subset_pathway_matrix\n",
        "\n",
        "\n",
        "def summarize_connections(*matrices):\n",
        "    connection_counts = [int(matrix.sum().sum()) for matrix in matrices]\n",
        "    labels = [\"SNP-Methylation\", \"Methylation-Gene\", \"Gene-Pathway\"]\n",
        "    for label, count in zip(labels, connection_counts):\n",
        "        print(f\"Total connections ({label}): {count}\")\n",
        "\n",
        "def build_edge_list(subset_methy_matrix, subset_gene_matrix, subset_pathway_matrix):\n",
        "    # SNP → Methylation edges\n",
        "    edges_snp_methy = (\n",
        "        subset_methy_matrix[subset_methy_matrix == 1]\n",
        "        .stack()\n",
        "        .reset_index()\n",
        "    )\n",
        "    edges_snp_methy.columns = [\"source\", \"target\", \"value\"]\n",
        "    edges_snp_methy[\"layer\"] = \"snp_methy\"\n",
        "\n",
        "    # Methylation → Gene edges\n",
        "    edges_methy_gene = (\n",
        "        subset_gene_matrix[subset_gene_matrix == 1]\n",
        "        .stack()\n",
        "        .reset_index()\n",
        "    )\n",
        "    edges_methy_gene.columns = [\"source\", \"target\", \"value\"]\n",
        "    edges_methy_gene[\"layer\"] = \"methy_gene\"\n",
        "\n",
        "    # Gene → GO term edges\n",
        "    edges_gene_go = (\n",
        "        subset_pathway_matrix[subset_pathway_matrix == 1]\n",
        "        .stack()\n",
        "        .reset_index()\n",
        "    )\n",
        "    edges_gene_go.columns = [\"source\", \"target\", \"value\"]\n",
        "    edges_gene_go[\"layer\"] = \"gene_go\"\n",
        "\n",
        "    # Combine all edges\n",
        "    edges_all = pd.concat(\n",
        "        [edges_snp_methy, edges_methy_gene, edges_gene_go],\n",
        "        ignore_index=True,\n",
        "    )\n",
        "\n",
        "    edges_all[\"value\"] = 1\n",
        "\n",
        "    return edges_all\n",
        "\n",
        "def plot_sankey_from_edges(edges_all):\n",
        "    edges_all_filtered = edges_all.copy()\n",
        "\n",
        "    # Get all unique nodes\n",
        "    nodes = pd.unique(edges_all_filtered[[\"source\", \"target\"]].values.ravel())\n",
        "\n",
        "    # Assign node categories\n",
        "    snps = [\n",
        "        node for node in nodes\n",
        "        if ((node.startswith(\"rs\") or \":\" in node) and not node.startswith(\"GO\"))\n",
        "    ]\n",
        "    methylation = [node for node in nodes if node.startswith(\"cg\")]\n",
        "    genes = [node for node in nodes if \"_at\" in node]\n",
        "    go_terms = [node for node in nodes if node.startswith(\"GO:\")]\n",
        "\n",
        "    # Define node order (left → right)\n",
        "    ordered_nodes = snps + methylation + genes + go_terms\n",
        "\n",
        "    # Map node name → index\n",
        "    node_indices = {name: i for i, name in enumerate(ordered_nodes)}\n",
        "\n",
        "    # Keep only edges where both nodes are in ordered_nodes\n",
        "    edges_all_filtered = edges_all_filtered[\n",
        "        edges_all_filtered[\"source\"].isin(ordered_nodes)\n",
        "        & edges_all_filtered[\"target\"].isin(ordered_nodes)\n",
        "    ].copy()\n",
        "\n",
        "    edges_all_filtered[\"source_index\"] = edges_all_filtered[\"source\"].map(node_indices)\n",
        "    edges_all_filtered[\"target_index\"] = edges_all_filtered[\"target\"].map(node_indices)\n",
        "\n",
        "    # x positions by category (normalized 0–1)\n",
        "    node_positions_x = [\n",
        "        0.0 if node in snps\n",
        "        else 0.33 if node in methylation\n",
        "        else 0.66 if node in genes\n",
        "        else 0.99\n",
        "        for node in ordered_nodes\n",
        "    ]\n",
        "\n",
        "    # Colors for nodes\n",
        "    unique_colors = pc.qualitative.Dark24\n",
        "    repeated_colors = (unique_colors * ((len(ordered_nodes) // len(unique_colors)) + 1))[:len(ordered_nodes)]\n",
        "    node_colors = repeated_colors\n",
        "\n",
        "    fig = go.Figure(go.Sankey(\n",
        "        arrangement=\"snap\",\n",
        "        node=dict(\n",
        "            pad=10,\n",
        "            thickness=20,\n",
        "            line=dict(color=\"black\", width=0.5),\n",
        "            label=ordered_nodes,\n",
        "            color=node_colors,\n",
        "            x=node_positions_x,\n",
        "        ),\n",
        "        link=dict(\n",
        "            source=edges_all_filtered[\"source_index\"],\n",
        "            target=edges_all_filtered[\"target_index\"],\n",
        "            value=edges_all_filtered[\"value\"],\n",
        "        ),\n",
        "    ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        font_size=14,\n",
        "        height=1500,\n",
        "        width=2000,\n",
        "        # title_text=\"SNP → Methylation → Gene → GO Term Associations\",\n",
        "    )\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U99duG_SSCI_"
      },
      "outputs": [],
      "source": [
        "# --------------------------\n",
        "# SECTION: Torch HINN Model\n",
        "# --------------------------\n",
        "\n",
        "class HINN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        snp_dim,\n",
        "        methy_dim,\n",
        "        exp_dim,\n",
        "        demo_dim,\n",
        "        sparse_methy_tensor,\n",
        "        sparse_gene_tensor,\n",
        "        sparse_pathway_tensor,\n",
        "        dense_nodes_1=128,\n",
        "        drop_rate=0.7,\n",
        "        activation_function=\"sigmoid\",  # we will treat this as \"only sigmoid\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # --- First block: SNP -> Methy ---\n",
        "        self.primary1 = PrimaryInputLayer(\n",
        "            units=snp_dim,\n",
        "            output_dim=methy_dim,\n",
        "            activation=activation_function,  # \"sigmoid\"\n",
        "            mask=sparse_methy_tensor,\n",
        "        )\n",
        "        self.secondary1 = SecondaryInputLayer(units=methy_dim)\n",
        "        self.mult1 = MultiplicationInputLayer(\n",
        "            units=methy_dim,\n",
        "            activation=activation_function,  # \"sigmoid\"\n",
        "        )\n",
        "\n",
        "        self.snp_fc = nn.Linear(snp_dim, 20)  # con_cat_layer_first\n",
        "\n",
        "        # --- Second block: Methy -> Gene (expression) ---\n",
        "        self.primary2 = PrimaryInputLayer(\n",
        "            units=methy_dim,\n",
        "            output_dim=exp_dim,\n",
        "            activation=activation_function,  # \"sigmoid\"\n",
        "            mask=sparse_gene_tensor,\n",
        "        )\n",
        "        self.secondary2 = SecondaryInputLayer(units=exp_dim)\n",
        "        self.mult2 = MultiplicationInputLayer(\n",
        "            units=exp_dim,\n",
        "            activation=activation_function,  # \"sigmoid\"\n",
        "        )\n",
        "\n",
        "        self.mid_fc = nn.Linear(methy_dim + 20, 20)  # con_cat_layer_sec\n",
        "\n",
        "        # --- Third block: Gene -> Pathway ---\n",
        "        pathway_dim = sparse_pathway_tensor.shape[1]\n",
        "        self.primary3 = PrimaryInputLayer(\n",
        "            units=exp_dim,\n",
        "            output_dim=pathway_dim,\n",
        "            activation=activation_function,  # \"sigmoid\"\n",
        "            mask=sparse_pathway_tensor,\n",
        "        )\n",
        "        self.mid_fc2 = nn.Linear(exp_dim + 20, 20)  # con_cat_layer_third\n",
        "\n",
        "        # --- Dense \"custom_layers\" stack ---\n",
        "        custom_input_dim = pathway_dim + 20  # fourth_output + con_cat_layer_third\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(custom_input_dim)\n",
        "        self.fc1 = nn.Linear(custom_input_dim, dense_nodes_1)\n",
        "        self.drop1 = nn.Dropout(drop_rate)\n",
        "\n",
        "        self.bn2 = nn.BatchNorm1d(dense_nodes_1)\n",
        "        self.fc2 = nn.Linear(dense_nodes_1, dense_nodes_1)\n",
        "        self.drop2 = nn.Dropout(drop_rate)\n",
        "\n",
        "        self.bn3 = nn.BatchNorm1d(dense_nodes_1)\n",
        "        self.fc3 = nn.Linear(dense_nodes_1, dense_nodes_1)\n",
        "        self.drop3 = nn.Dropout(drop_rate)\n",
        "\n",
        "        self.bn4 = nn.BatchNorm1d(dense_nodes_1)\n",
        "        self.fc4 = nn.Linear(dense_nodes_1, dense_nodes_1)\n",
        "        self.drop4 = nn.Dropout(drop_rate)\n",
        "\n",
        "        self.dense_fourth = nn.Linear(dense_nodes_1, 20)  # dense_fourth\n",
        "        self.bn_demo = nn.BatchNorm1d(20 + demo_dim)\n",
        "        self.fc_demo = nn.Linear(20 + demo_dim, dense_nodes_1)\n",
        "        self.drop_demo = nn.Dropout(drop_rate)\n",
        "\n",
        "        self.out = nn.Linear(dense_nodes_1, 1)  # final output\n",
        "\n",
        "        # store activation choice, but we'll only use sigmoid currently\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "    def _nonlin(self, x):\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "    def forward(self, snp, methy, exp, demo):\n",
        "        # --- First block ---\n",
        "        primary1 = self.primary1(snp)\n",
        "        secondary1 = self.secondary1(methy)\n",
        "        mult_res1 = primary1 * secondary1\n",
        "        mult1 = self.mult1(mult_res1)\n",
        "\n",
        "        snp_fc = self._nonlin(self.snp_fc(snp))  # sigmoid(snp_fc)\n",
        "        out2 = torch.cat([mult1, snp_fc], dim=1)\n",
        "\n",
        "        # --- Second block ---\n",
        "        primary2 = self.primary2(mult1)\n",
        "        secondary2 = self.secondary2(exp)\n",
        "\n",
        "        eps = 1e-6\n",
        "        denom = primary2.clone()\n",
        "        denom = torch.where(denom.abs() < eps, eps * torch.ones_like(denom), denom)\n",
        "\n",
        "        div_res1 = secondary2 / denom\n",
        "        div_res1 = torch.clamp(div_res1, -1e6, 1e6)\n",
        "\n",
        "        mult2 = self.mult2(div_res1)\n",
        "\n",
        "        mid_fc = self._nonlin(self.mid_fc(out2))\n",
        "        out3 = torch.cat([mult2, mid_fc], dim=1)\n",
        "\n",
        "        # --- Third block ---\n",
        "        primary3 = self.primary3(mult2)\n",
        "        mid_fc2 = self._nonlin(self.mid_fc2(out3))\n",
        "        out4 = torch.cat([primary3, mid_fc2], dim=1)\n",
        "\n",
        "        # --- Dense stack similar to custom_layers ---\n",
        "        x = self.bn1(out4)\n",
        "        x = self._nonlin(self.fc1(x))\n",
        "        x = self.drop1(x)\n",
        "\n",
        "        x = self.bn2(x)\n",
        "        x = self._nonlin(self.fc2(x))\n",
        "        x = self.drop2(x)\n",
        "\n",
        "        x = self.bn3(x)\n",
        "        x = self._nonlin(self.fc3(x))\n",
        "        x = self.drop3(x)\n",
        "\n",
        "        x = self.bn4(x)\n",
        "        x = self._nonlin(self.fc4(x))\n",
        "        x = self.drop4(x)\n",
        "\n",
        "        dense_fourth = self._nonlin(self.dense_fourth(x))\n",
        "        demo_concat = torch.cat([dense_fourth, demo], dim=1)\n",
        "\n",
        "        x = self.bn_demo(demo_concat)\n",
        "        x = self._nonlin(self.fc_demo(x))\n",
        "        x = self.drop_demo(x)\n",
        "\n",
        "        out = self.out(x)\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IDDpwbKtSCjf"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# SECTION: Execution Pipeline\n",
        "# ---------------------------\n",
        "\n",
        "def main():\n",
        "    device = \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Data & splits\n",
        "    # ---------------------------\n",
        "    data = load_and_process_data()\n",
        "    y = data[\"MMSE_label\"]\n",
        "    X = data.drop(columns=[c for c in data.columns if c.endswith(\"MMSE_label\")])\n",
        "\n",
        "    # train / test split\n",
        "    X_train_int, X_test_df, y_train_int, y_test = train_test_split(\n",
        "        X, y, test_size=0.3\n",
        "    )\n",
        "    # train / val split\n",
        "    X_train_df, X_val_df, y_train, y_val = train_test_split(\n",
        "        X_train_int, y_train_int, test_size=0.2\n",
        "    )\n",
        "\n",
        "    # TRAIN\n",
        "    X_train_snp = X_train_df.filter(like=\"_snp\").values\n",
        "    X_train_methy = X_train_df.filter(like=\"_methy\").values\n",
        "    X_train_exp = X_train_df.filter(like=\"_expression\").values\n",
        "    X_train_demo = X_train_df.filter(like=\"_demograph\").values\n",
        "\n",
        "    X_train_list = [\n",
        "        torch.tensor(X_train_snp, dtype=torch.float32),\n",
        "        torch.tensor(X_train_methy, dtype=torch.float32),\n",
        "        torch.tensor(X_train_exp, dtype=torch.float32),\n",
        "        torch.tensor(X_train_demo, dtype=torch.float32),\n",
        "    ]\n",
        "    y_train_t = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "\n",
        "    # VAL\n",
        "    X_val_snp = X_val_df.filter(like=\"_snp\").values\n",
        "    X_val_methy = X_val_df.filter(like=\"_methy\").values\n",
        "    X_val_exp = X_val_df.filter(like=\"_expression\").values\n",
        "    X_val_demo = X_val_df.filter(like=\"_demograph\").values\n",
        "\n",
        "    X_val_list = [\n",
        "        torch.tensor(X_val_snp, dtype=torch.float32),\n",
        "        torch.tensor(X_val_methy, dtype=torch.float32),\n",
        "        torch.tensor(X_val_exp, dtype=torch.float32),\n",
        "        torch.tensor(X_val_demo, dtype=torch.float32),\n",
        "    ]\n",
        "    y_val_t = torch.tensor(y_val.values, dtype=torch.float32)\n",
        "\n",
        "    # TEST\n",
        "    X_test_snp = X_test_df.filter(like=\"_snp\").values\n",
        "    X_test_methy = X_test_df.filter(like=\"_methy\").values\n",
        "    X_test_exp = X_test_df.filter(like=\"_expression\").values\n",
        "    X_test_demo = X_test_df.filter(like=\"_demograph\").values\n",
        "\n",
        "    X_test_list = [\n",
        "        torch.tensor(X_test_snp, dtype=torch.float32),\n",
        "        torch.tensor(X_test_methy, dtype=torch.float32),\n",
        "        torch.tensor(X_test_exp, dtype=torch.float32),\n",
        "        torch.tensor(X_test_demo, dtype=torch.float32),\n",
        "    ]\n",
        "    y_test_t = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Datasets & loaders\n",
        "    # ---------------------------\n",
        "    train_dataset = CustomDataset(X_train_list, y_train_t)\n",
        "    val_dataset = CustomDataset(X_val_list, y_val_t)\n",
        "    test_dataset = CustomDataset(X_test_list, y_test_t)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Sparse matrices -> tensors\n",
        "    # ---------------------------\n",
        "    sparse_methy = pd.read_csv(\"snp_methyl_matrix.csv\", index_col=0)\n",
        "    sparse_gene = pd.read_csv(\"methyl_gene_matrix.csv.zip\", compression='zip', index_col=0)\n",
        "    sparse_pathway = pd.read_csv(\"gene_pathway_matrix.csv\", index_col=0)\n",
        "\n",
        "    sparse_methy_tensor = torch.tensor(sparse_methy.values, dtype=torch.float32)\n",
        "    sparse_gene_tensor = torch.tensor(sparse_gene.values, dtype=torch.float32)\n",
        "    sparse_pathway_tensor = torch.tensor(sparse_pathway.values, dtype=torch.float32)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Model init\n",
        "    # ---------------------------\n",
        "    snp_dim = X_train_snp.shape[1]\n",
        "    methy_dim = X_train_methy.shape[1]\n",
        "    exp_dim = X_train_exp.shape[1]\n",
        "    demo_dim = X_train_demo.shape[1]\n",
        "\n",
        "    activation_function = \"sigmoid\"\n",
        "    dense_nodes_1 = 128\n",
        "    drop_rate = 0.7\n",
        "\n",
        "    model = HINN(\n",
        "        snp_dim=snp_dim,\n",
        "        methy_dim=methy_dim,\n",
        "        exp_dim=exp_dim,\n",
        "        demo_dim=demo_dim,\n",
        "        sparse_methy_tensor=sparse_methy_tensor,\n",
        "        sparse_gene_tensor=sparse_gene_tensor,\n",
        "        sparse_pathway_tensor=sparse_pathway_tensor,\n",
        "        dense_nodes_1=dense_nodes_1,\n",
        "        drop_rate=drop_rate,\n",
        "        activation_function=activation_function,\n",
        "    )\n",
        "\n",
        "    # ---------------------------\n",
        "    # Train\n",
        "    # ---------------------------\n",
        "    model = train_model_torch(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        device=device,\n",
        "        lr=1e-3,\n",
        "        epochs=1000,\n",
        "        patience=50,\n",
        "    )\n",
        "\n",
        "    # ---------------------------\n",
        "    # Evaluate on TRUE test set\n",
        "    # ---------------------------\n",
        "    eval_results = evaluate_model_torch(model, test_loader, device=device)\n",
        "    print(\"MAE (Test):\", eval_results[\"mae\"])\n",
        "    print(\"MSE (Test):\", eval_results[\"mse\"])\n",
        "\n",
        "    # ---------------------------\n",
        "    # Captum: DeepLift attributions\n",
        "    # ---------------------------\n",
        "\n",
        "    test_inputs = tuple(\n",
        "        torch.tensor(arr, dtype=torch.float32, requires_grad=True).to(device)\n",
        "        for arr in [X_test_snp, X_test_methy, X_test_exp, X_test_demo]\n",
        "    )\n",
        "\n",
        "    baselines = tuple(\n",
        "        torch.tensor(arr.mean(axis=0), dtype=torch.float32)\n",
        "        .unsqueeze(0)\n",
        "        .expand_as(torch.tensor(arr, dtype=torch.float32))\n",
        "        .to(device)\n",
        "        for arr in [X_test_snp, X_test_methy, X_test_exp, X_test_demo]\n",
        "    )\n",
        "\n",
        "    attributions = interpret_model(model, test_inputs, baselines, device=device)\n",
        "    attr_snp, attr_methy, attr_gene, attr_demo = attributions\n",
        "\n",
        "    # Mean absolute DeepLift attribution per feature\n",
        "    snp_importance   = attr_snp.abs().mean(dim=0).detach().cpu().numpy()\n",
        "    methy_importance = attr_methy.abs().mean(dim=0).detach().cpu().numpy()\n",
        "    gene_importance  = attr_gene.abs().mean(dim=0).detach().cpu().numpy()\n",
        "\n",
        "    # ---------------------------\n",
        "    # Attribution export\n",
        "    # ---------------------------\n",
        "    feature_names = [\n",
        "        X_train_df.filter(like=s).columns.tolist()\n",
        "        for s in [\"_snp\", \"_methy\", \"_expression\", \"_demograph\"]\n",
        "    ]\n",
        "    export_attributions(attributions, feature_names, \"~/MMSE\")\n",
        "\n",
        "    snp_feature_names   = feature_names[0]  # _snp\n",
        "    methy_feature_names = feature_names[1]  # _methy\n",
        "    gene_feature_names  = feature_names[2]  # _expression\n",
        "\n",
        "    # Top-k per modality\n",
        "    TOP_SNP   = 20\n",
        "    TOP_METHY = 100\n",
        "    TOP_GENE  = 50\n",
        "\n",
        "    top_snp_idx   = np.argsort(-snp_importance)[:TOP_SNP]\n",
        "    top_methy_idx = np.argsort(-methy_importance)[:TOP_METHY]\n",
        "    top_gene_idx  = np.argsort(-gene_importance)[:TOP_GENE]\n",
        "\n",
        "    snp_list = [\n",
        "        snp_feature_names[i].replace(\"_snp\", \"\")\n",
        "        for i in top_snp_idx\n",
        "    ]\n",
        "    methy_list = [\n",
        "        methy_feature_names[i].replace(\"_methy\", \"\")\n",
        "        for i in top_methy_idx\n",
        "    ]\n",
        "    gene_list = [\n",
        "        gene_feature_names[i].replace(\"_expression\", \"\")\n",
        "        for i in top_gene_idx\n",
        "    ]\n",
        "\n",
        "    subset_methy_matrix, subset_gene_matrix, subset_pathway_matrix = filter_matrices_by_top_features(\n",
        "    snp_list, methy_list, gene_list, sparse_methy, sparse_gene, sparse_pathway\n",
        "    )\n",
        "\n",
        "    summarize_connections(subset_methy_matrix, subset_gene_matrix, subset_pathway_matrix)\n",
        "\n",
        "    # Build edge list and plot old-style Sankey\n",
        "    edges_all = build_edge_list(\n",
        "        subset_methy_matrix,\n",
        "        subset_gene_matrix,\n",
        "        subset_pathway_matrix,\n",
        "    )\n",
        "\n",
        "    plot_sankey_from_edges(edges_all)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "drD-QF_sSOgG",
        "outputId": "be8c14fe-3f39-4cdd-eb11-22bb77296ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Epoch 001 | train_loss=22.8188 | val_loss=23.9323\n",
            "Epoch 002 | train_loss=22.7099 | val_loss=23.8956\n",
            "Epoch 003 | train_loss=22.7994 | val_loss=23.8398\n",
            "Epoch 004 | train_loss=22.6633 | val_loss=23.7851\n",
            "Epoch 005 | train_loss=22.4898 | val_loss=23.7273\n",
            "Epoch 006 | train_loss=22.4520 | val_loss=23.6678\n",
            "Epoch 007 | train_loss=22.3620 | val_loss=23.6073\n",
            "Epoch 008 | train_loss=22.3582 | val_loss=23.5465\n",
            "Epoch 009 | train_loss=22.2510 | val_loss=23.4843\n",
            "Epoch 010 | train_loss=22.2815 | val_loss=23.4219\n",
            "Epoch 011 | train_loss=21.9768 | val_loss=23.3589\n",
            "Epoch 012 | train_loss=22.1409 | val_loss=23.2955\n",
            "Epoch 013 | train_loss=22.1049 | val_loss=23.2325\n",
            "Epoch 014 | train_loss=21.9520 | val_loss=23.1691\n",
            "Epoch 015 | train_loss=21.9658 | val_loss=23.1049\n",
            "Epoch 016 | train_loss=21.6508 | val_loss=23.0409\n",
            "Epoch 017 | train_loss=21.8606 | val_loss=22.9769\n",
            "Epoch 018 | train_loss=21.4737 | val_loss=22.9126\n",
            "Epoch 019 | train_loss=21.8541 | val_loss=22.8485\n",
            "Epoch 020 | train_loss=21.7148 | val_loss=22.7848\n",
            "Epoch 021 | train_loss=21.7944 | val_loss=22.7207\n",
            "Epoch 022 | train_loss=21.3485 | val_loss=22.6567\n",
            "Epoch 023 | train_loss=21.3132 | val_loss=22.5920\n",
            "Epoch 024 | train_loss=21.2729 | val_loss=22.5274\n",
            "Epoch 025 | train_loss=21.4120 | val_loss=22.4623\n",
            "Epoch 026 | train_loss=21.2898 | val_loss=22.3979\n",
            "Epoch 027 | train_loss=21.3488 | val_loss=22.3339\n",
            "Epoch 028 | train_loss=21.3457 | val_loss=22.2696\n",
            "Epoch 029 | train_loss=20.9719 | val_loss=22.2059\n",
            "Epoch 030 | train_loss=20.9546 | val_loss=22.1420\n",
            "Epoch 031 | train_loss=21.0259 | val_loss=22.0781\n",
            "Epoch 032 | train_loss=21.0781 | val_loss=22.0143\n",
            "Epoch 033 | train_loss=20.5912 | val_loss=21.9500\n",
            "Epoch 034 | train_loss=20.6878 | val_loss=21.8857\n",
            "Epoch 035 | train_loss=20.7916 | val_loss=21.8209\n",
            "Epoch 036 | train_loss=20.7083 | val_loss=21.7568\n",
            "Epoch 037 | train_loss=20.5989 | val_loss=21.6924\n",
            "Epoch 038 | train_loss=20.7293 | val_loss=21.6281\n",
            "Epoch 039 | train_loss=20.2874 | val_loss=21.5640\n",
            "Epoch 040 | train_loss=20.4298 | val_loss=21.5017\n",
            "Epoch 041 | train_loss=20.5208 | val_loss=21.4381\n",
            "Epoch 042 | train_loss=20.0548 | val_loss=21.3733\n",
            "Epoch 043 | train_loss=20.2805 | val_loss=21.3086\n",
            "Epoch 044 | train_loss=20.2269 | val_loss=21.2461\n",
            "Epoch 045 | train_loss=19.8921 | val_loss=21.1804\n",
            "Epoch 046 | train_loss=19.8173 | val_loss=21.1153\n",
            "Epoch 047 | train_loss=20.0937 | val_loss=21.0518\n",
            "Epoch 048 | train_loss=19.7953 | val_loss=20.9879\n",
            "Epoch 049 | train_loss=19.9669 | val_loss=20.9236\n",
            "Epoch 050 | train_loss=19.9681 | val_loss=20.8608\n",
            "Epoch 051 | train_loss=19.6346 | val_loss=20.7987\n",
            "Epoch 052 | train_loss=19.8244 | val_loss=20.7341\n",
            "Epoch 053 | train_loss=19.9423 | val_loss=20.6705\n",
            "Epoch 054 | train_loss=19.1782 | val_loss=20.6056\n",
            "Epoch 055 | train_loss=19.4598 | val_loss=20.5405\n",
            "Epoch 056 | train_loss=19.2129 | val_loss=20.4752\n",
            "Epoch 057 | train_loss=19.1629 | val_loss=20.4074\n",
            "Epoch 058 | train_loss=19.0507 | val_loss=20.3382\n",
            "Epoch 059 | train_loss=19.0070 | val_loss=20.2730\n",
            "Epoch 060 | train_loss=19.0581 | val_loss=20.2077\n",
            "Epoch 061 | train_loss=19.2444 | val_loss=20.1407\n",
            "Epoch 062 | train_loss=18.9120 | val_loss=20.0742\n",
            "Epoch 063 | train_loss=18.7225 | val_loss=20.0057\n",
            "Epoch 064 | train_loss=18.7040 | val_loss=19.9390\n",
            "Epoch 065 | train_loss=18.8976 | val_loss=19.8709\n",
            "Epoch 066 | train_loss=18.5837 | val_loss=19.8043\n",
            "Epoch 067 | train_loss=18.6883 | val_loss=19.7397\n",
            "Epoch 068 | train_loss=18.5277 | val_loss=19.6687\n",
            "Epoch 069 | train_loss=18.3764 | val_loss=19.6008\n",
            "Epoch 070 | train_loss=18.1649 | val_loss=19.5334\n",
            "Epoch 071 | train_loss=18.3515 | val_loss=19.4689\n",
            "Epoch 072 | train_loss=17.9198 | val_loss=19.4013\n",
            "Epoch 073 | train_loss=17.9888 | val_loss=19.3351\n",
            "Epoch 074 | train_loss=18.3242 | val_loss=19.2641\n",
            "Epoch 075 | train_loss=18.0129 | val_loss=19.1967\n",
            "Epoch 076 | train_loss=18.0864 | val_loss=19.1301\n",
            "Epoch 077 | train_loss=17.7335 | val_loss=19.0647\n",
            "Epoch 078 | train_loss=17.9417 | val_loss=18.9903\n",
            "Epoch 079 | train_loss=17.8745 | val_loss=18.9263\n",
            "Epoch 080 | train_loss=18.0066 | val_loss=18.8556\n",
            "Epoch 081 | train_loss=17.6260 | val_loss=18.7853\n",
            "Epoch 082 | train_loss=17.9569 | val_loss=18.7220\n",
            "Epoch 083 | train_loss=17.5689 | val_loss=18.6517\n",
            "Epoch 084 | train_loss=17.9637 | val_loss=18.5845\n",
            "Epoch 085 | train_loss=17.3362 | val_loss=18.5109\n",
            "Epoch 086 | train_loss=17.2117 | val_loss=18.4407\n",
            "Epoch 087 | train_loss=17.4155 | val_loss=18.3758\n",
            "Epoch 088 | train_loss=17.0890 | val_loss=18.3043\n",
            "Epoch 089 | train_loss=17.0608 | val_loss=18.2340\n",
            "Epoch 090 | train_loss=17.1926 | val_loss=18.1642\n",
            "Epoch 091 | train_loss=16.8090 | val_loss=18.0911\n",
            "Epoch 092 | train_loss=16.7532 | val_loss=18.0255\n",
            "Epoch 093 | train_loss=16.8078 | val_loss=17.9541\n",
            "Epoch 094 | train_loss=16.7462 | val_loss=17.8851\n",
            "Epoch 095 | train_loss=16.7004 | val_loss=17.8194\n",
            "Epoch 096 | train_loss=16.1849 | val_loss=17.7457\n",
            "Epoch 097 | train_loss=16.8149 | val_loss=17.6757\n",
            "Epoch 098 | train_loss=16.4552 | val_loss=17.5980\n",
            "Epoch 099 | train_loss=16.1135 | val_loss=17.5313\n",
            "Epoch 100 | train_loss=16.6822 | val_loss=17.4544\n",
            "Epoch 101 | train_loss=16.1618 | val_loss=17.3785\n",
            "Epoch 102 | train_loss=16.1588 | val_loss=17.3079\n",
            "Epoch 103 | train_loss=16.1121 | val_loss=17.2371\n",
            "Epoch 104 | train_loss=15.9646 | val_loss=17.1721\n",
            "Epoch 105 | train_loss=16.0464 | val_loss=17.1016\n",
            "Epoch 106 | train_loss=15.8021 | val_loss=17.0254\n",
            "Epoch 107 | train_loss=15.9765 | val_loss=16.9612\n",
            "Epoch 108 | train_loss=15.4304 | val_loss=16.8778\n",
            "Epoch 109 | train_loss=16.1233 | val_loss=16.7953\n",
            "Epoch 110 | train_loss=15.5295 | val_loss=16.7249\n",
            "Epoch 111 | train_loss=15.4679 | val_loss=16.6449\n",
            "Epoch 112 | train_loss=15.3249 | val_loss=16.5732\n",
            "Epoch 113 | train_loss=15.2658 | val_loss=16.4985\n",
            "Epoch 114 | train_loss=15.1461 | val_loss=16.4168\n",
            "Epoch 115 | train_loss=15.0939 | val_loss=16.3279\n",
            "Epoch 116 | train_loss=15.5059 | val_loss=16.2549\n",
            "Epoch 117 | train_loss=14.9501 | val_loss=16.1681\n",
            "Epoch 118 | train_loss=14.8859 | val_loss=16.0881\n",
            "Epoch 119 | train_loss=14.9992 | val_loss=16.0124\n",
            "Epoch 120 | train_loss=14.5505 | val_loss=15.9229\n",
            "Epoch 121 | train_loss=14.4606 | val_loss=15.8376\n",
            "Epoch 122 | train_loss=14.5768 | val_loss=15.7451\n",
            "Epoch 123 | train_loss=14.7733 | val_loss=15.6769\n",
            "Epoch 124 | train_loss=14.6276 | val_loss=15.5896\n",
            "Epoch 125 | train_loss=14.4005 | val_loss=15.5079\n",
            "Epoch 126 | train_loss=14.2184 | val_loss=15.4103\n",
            "Epoch 127 | train_loss=13.6312 | val_loss=15.3270\n",
            "Epoch 128 | train_loss=13.8145 | val_loss=15.2543\n",
            "Epoch 129 | train_loss=13.9620 | val_loss=15.1755\n",
            "Epoch 130 | train_loss=13.8598 | val_loss=15.0894\n",
            "Epoch 131 | train_loss=13.6598 | val_loss=14.9826\n",
            "Epoch 132 | train_loss=13.3508 | val_loss=14.8931\n",
            "Epoch 133 | train_loss=13.8786 | val_loss=14.7970\n",
            "Epoch 134 | train_loss=13.8115 | val_loss=14.6951\n",
            "Epoch 135 | train_loss=13.4084 | val_loss=14.5912\n",
            "Epoch 136 | train_loss=13.6136 | val_loss=14.5036\n",
            "Epoch 137 | train_loss=13.1098 | val_loss=14.4007\n",
            "Epoch 138 | train_loss=13.3651 | val_loss=14.3185\n",
            "Epoch 139 | train_loss=12.7950 | val_loss=14.2253\n",
            "Epoch 140 | train_loss=12.5537 | val_loss=14.1353\n",
            "Epoch 141 | train_loss=12.9355 | val_loss=14.0411\n",
            "Epoch 142 | train_loss=12.8858 | val_loss=13.9420\n",
            "Epoch 143 | train_loss=12.6870 | val_loss=13.8248\n",
            "Epoch 144 | train_loss=12.8320 | val_loss=13.7315\n",
            "Epoch 145 | train_loss=12.4735 | val_loss=13.6254\n",
            "Epoch 146 | train_loss=12.6788 | val_loss=13.5056\n",
            "Epoch 147 | train_loss=12.4685 | val_loss=13.3953\n",
            "Epoch 148 | train_loss=12.0417 | val_loss=13.2832\n",
            "Epoch 149 | train_loss=11.9531 | val_loss=13.2024\n",
            "Epoch 150 | train_loss=11.8971 | val_loss=13.1092\n",
            "Epoch 151 | train_loss=11.6745 | val_loss=13.0328\n",
            "Epoch 152 | train_loss=12.1026 | val_loss=12.9241\n",
            "Epoch 153 | train_loss=11.6631 | val_loss=12.7909\n",
            "Epoch 154 | train_loss=11.8463 | val_loss=12.7051\n",
            "Epoch 155 | train_loss=11.4368 | val_loss=12.6005\n",
            "Epoch 156 | train_loss=11.7059 | val_loss=12.5129\n",
            "Epoch 157 | train_loss=10.8025 | val_loss=12.4115\n",
            "Epoch 158 | train_loss=11.2055 | val_loss=12.3057\n",
            "Epoch 159 | train_loss=11.1506 | val_loss=12.2553\n",
            "Epoch 160 | train_loss=11.1159 | val_loss=12.1498\n",
            "Epoch 161 | train_loss=10.5398 | val_loss=12.0606\n",
            "Epoch 162 | train_loss=10.8570 | val_loss=11.9295\n",
            "Epoch 163 | train_loss=10.5879 | val_loss=11.7952\n",
            "Epoch 164 | train_loss=10.5324 | val_loss=11.7222\n",
            "Epoch 165 | train_loss=10.1239 | val_loss=11.6610\n",
            "Epoch 166 | train_loss=9.7663 | val_loss=11.5378\n",
            "Epoch 167 | train_loss=9.8844 | val_loss=11.4214\n",
            "Epoch 168 | train_loss=10.0516 | val_loss=11.3188\n",
            "Epoch 169 | train_loss=9.5150 | val_loss=11.2247\n",
            "Epoch 170 | train_loss=8.8054 | val_loss=11.0864\n",
            "Epoch 171 | train_loss=9.2935 | val_loss=10.9595\n",
            "Epoch 172 | train_loss=9.5201 | val_loss=10.7772\n",
            "Epoch 173 | train_loss=8.5917 | val_loss=10.6061\n",
            "Epoch 174 | train_loss=9.2310 | val_loss=10.4904\n",
            "Epoch 175 | train_loss=9.1824 | val_loss=10.3335\n",
            "Epoch 176 | train_loss=8.6094 | val_loss=10.1921\n",
            "Epoch 177 | train_loss=9.0071 | val_loss=10.0547\n",
            "Epoch 178 | train_loss=8.9906 | val_loss=9.9392\n",
            "Epoch 179 | train_loss=8.5130 | val_loss=9.7265\n",
            "Epoch 180 | train_loss=8.0173 | val_loss=9.5953\n",
            "Epoch 181 | train_loss=8.6620 | val_loss=9.4060\n",
            "Epoch 182 | train_loss=7.8758 | val_loss=9.2527\n",
            "Epoch 183 | train_loss=7.9021 | val_loss=9.1498\n",
            "Epoch 184 | train_loss=7.8946 | val_loss=9.0163\n",
            "Epoch 185 | train_loss=7.4183 | val_loss=8.8917\n",
            "Epoch 186 | train_loss=7.9885 | val_loss=8.7732\n",
            "Epoch 187 | train_loss=7.3766 | val_loss=8.6201\n",
            "Epoch 188 | train_loss=6.9970 | val_loss=8.4492\n",
            "Epoch 189 | train_loss=6.4166 | val_loss=8.3365\n",
            "Epoch 190 | train_loss=7.5607 | val_loss=8.2130\n",
            "Epoch 191 | train_loss=7.0481 | val_loss=8.0840\n",
            "Epoch 192 | train_loss=6.5869 | val_loss=8.0095\n",
            "Epoch 193 | train_loss=6.2609 | val_loss=7.8503\n",
            "Epoch 194 | train_loss=6.5321 | val_loss=7.7609\n",
            "Epoch 195 | train_loss=5.9411 | val_loss=7.6395\n",
            "Epoch 196 | train_loss=6.4007 | val_loss=7.5650\n",
            "Epoch 197 | train_loss=6.0088 | val_loss=7.4083\n",
            "Epoch 198 | train_loss=5.9729 | val_loss=7.3391\n",
            "Epoch 199 | train_loss=6.1211 | val_loss=7.2360\n",
            "Epoch 200 | train_loss=5.9337 | val_loss=7.2260\n",
            "Epoch 201 | train_loss=6.3654 | val_loss=7.1542\n",
            "Epoch 202 | train_loss=5.7852 | val_loss=7.0586\n",
            "Epoch 203 | train_loss=4.8238 | val_loss=6.9300\n",
            "Epoch 204 | train_loss=5.2505 | val_loss=6.7703\n",
            "Epoch 205 | train_loss=4.5486 | val_loss=6.6517\n",
            "Epoch 206 | train_loss=5.1483 | val_loss=6.5050\n",
            "Epoch 207 | train_loss=5.1844 | val_loss=6.2664\n",
            "Epoch 208 | train_loss=4.6768 | val_loss=6.1343\n",
            "Epoch 209 | train_loss=4.6348 | val_loss=5.9639\n",
            "Epoch 210 | train_loss=4.4572 | val_loss=5.9088\n",
            "Epoch 211 | train_loss=4.7959 | val_loss=5.7995\n",
            "Epoch 212 | train_loss=4.9065 | val_loss=5.6740\n",
            "Epoch 213 | train_loss=3.8699 | val_loss=5.5542\n",
            "Epoch 214 | train_loss=4.4344 | val_loss=5.4357\n",
            "Epoch 215 | train_loss=5.0402 | val_loss=5.3352\n",
            "Epoch 216 | train_loss=4.8373 | val_loss=5.1510\n",
            "Epoch 217 | train_loss=4.2984 | val_loss=5.0237\n",
            "Epoch 218 | train_loss=4.2231 | val_loss=4.9014\n",
            "Epoch 219 | train_loss=3.5836 | val_loss=4.7998\n",
            "Epoch 220 | train_loss=4.2763 | val_loss=4.6761\n",
            "Epoch 221 | train_loss=3.6933 | val_loss=4.4224\n",
            "Epoch 222 | train_loss=4.7441 | val_loss=4.1248\n",
            "Epoch 223 | train_loss=3.5031 | val_loss=3.8489\n",
            "Epoch 224 | train_loss=4.3308 | val_loss=3.6261\n",
            "Epoch 225 | train_loss=3.9683 | val_loss=3.4741\n",
            "Epoch 226 | train_loss=3.9923 | val_loss=3.3145\n",
            "Epoch 227 | train_loss=3.6871 | val_loss=3.2330\n",
            "Epoch 228 | train_loss=4.0256 | val_loss=3.1419\n",
            "Epoch 229 | train_loss=4.5710 | val_loss=3.0747\n",
            "Epoch 230 | train_loss=4.7562 | val_loss=3.0430\n",
            "Epoch 231 | train_loss=3.7023 | val_loss=2.9754\n",
            "Epoch 232 | train_loss=3.9792 | val_loss=2.9169\n",
            "Epoch 233 | train_loss=3.3370 | val_loss=2.8653\n",
            "Epoch 234 | train_loss=4.6004 | val_loss=2.8184\n",
            "Epoch 235 | train_loss=3.7771 | val_loss=2.7693\n",
            "Epoch 236 | train_loss=3.6844 | val_loss=2.7446\n",
            "Epoch 237 | train_loss=3.5554 | val_loss=2.7209\n",
            "Epoch 238 | train_loss=4.0494 | val_loss=2.7226\n",
            "Epoch 239 | train_loss=4.4500 | val_loss=2.7374\n",
            "Epoch 240 | train_loss=4.9168 | val_loss=2.7446\n",
            "Epoch 241 | train_loss=3.8126 | val_loss=2.7953\n",
            "Epoch 242 | train_loss=4.4058 | val_loss=2.8097\n",
            "Epoch 243 | train_loss=3.7789 | val_loss=2.8567\n",
            "Epoch 244 | train_loss=4.2358 | val_loss=2.8677\n",
            "Epoch 245 | train_loss=3.8586 | val_loss=2.8886\n",
            "Epoch 246 | train_loss=3.1842 | val_loss=2.8812\n",
            "Epoch 247 | train_loss=4.4013 | val_loss=2.8874\n",
            "Epoch 248 | train_loss=3.6399 | val_loss=2.8838\n",
            "Epoch 249 | train_loss=3.7094 | val_loss=2.8969\n",
            "Epoch 250 | train_loss=4.7579 | val_loss=2.9063\n",
            "Epoch 251 | train_loss=3.7495 | val_loss=2.9066\n",
            "Epoch 252 | train_loss=4.1312 | val_loss=2.8974\n",
            "Epoch 253 | train_loss=2.7864 | val_loss=2.8865\n",
            "Epoch 254 | train_loss=3.4870 | val_loss=2.8619\n",
            "Epoch 255 | train_loss=3.6649 | val_loss=2.8639\n",
            "Epoch 256 | train_loss=3.8668 | val_loss=2.8527\n",
            "Epoch 257 | train_loss=3.4418 | val_loss=2.8608\n",
            "Epoch 258 | train_loss=3.3829 | val_loss=2.8455\n",
            "Epoch 259 | train_loss=3.6677 | val_loss=2.8309\n",
            "Epoch 260 | train_loss=3.4727 | val_loss=2.8117\n",
            "Epoch 261 | train_loss=3.7381 | val_loss=2.8102\n",
            "Epoch 262 | train_loss=3.6181 | val_loss=2.8329\n",
            "Epoch 263 | train_loss=3.1303 | val_loss=2.8307\n",
            "Epoch 264 | train_loss=3.5261 | val_loss=2.8316\n",
            "Epoch 265 | train_loss=4.6082 | val_loss=2.8513\n",
            "Epoch 266 | train_loss=3.5076 | val_loss=2.8291\n",
            "Epoch 267 | train_loss=3.7710 | val_loss=2.8294\n",
            "Epoch 268 | train_loss=4.8466 | val_loss=2.8221\n",
            "Epoch 269 | train_loss=3.4925 | val_loss=2.8245\n",
            "Epoch 270 | train_loss=4.1822 | val_loss=2.8105\n",
            "Epoch 271 | train_loss=3.8575 | val_loss=2.7691\n",
            "Epoch 272 | train_loss=3.5114 | val_loss=2.7228\n",
            "Epoch 273 | train_loss=3.7953 | val_loss=2.7100\n",
            "Epoch 274 | train_loss=4.4259 | val_loss=2.6827\n",
            "Epoch 275 | train_loss=3.6484 | val_loss=2.6624\n",
            "Epoch 276 | train_loss=3.7356 | val_loss=2.6541\n",
            "Epoch 277 | train_loss=4.2994 | val_loss=2.6402\n",
            "Epoch 278 | train_loss=4.4384 | val_loss=2.6512\n",
            "Epoch 279 | train_loss=4.1037 | val_loss=2.6747\n",
            "Epoch 280 | train_loss=4.4833 | val_loss=2.6716\n",
            "Epoch 281 | train_loss=4.0293 | val_loss=2.6815\n",
            "Epoch 282 | train_loss=4.2648 | val_loss=2.7098\n",
            "Epoch 283 | train_loss=3.8991 | val_loss=2.7272\n",
            "Epoch 284 | train_loss=4.2620 | val_loss=2.7433\n",
            "Epoch 285 | train_loss=4.3544 | val_loss=2.7596\n",
            "Epoch 286 | train_loss=4.0906 | val_loss=2.7740\n",
            "Epoch 287 | train_loss=4.8841 | val_loss=2.7906\n",
            "Epoch 288 | train_loss=3.3892 | val_loss=2.7938\n",
            "Epoch 289 | train_loss=4.1450 | val_loss=2.7839\n",
            "Epoch 290 | train_loss=4.2070 | val_loss=2.7777\n",
            "Epoch 291 | train_loss=3.3711 | val_loss=2.7868\n",
            "Epoch 292 | train_loss=4.3457 | val_loss=2.7993\n",
            "Epoch 293 | train_loss=3.5229 | val_loss=2.8066\n",
            "Epoch 294 | train_loss=3.5016 | val_loss=2.8098\n",
            "Epoch 295 | train_loss=3.7618 | val_loss=2.8096\n",
            "Epoch 296 | train_loss=3.6077 | val_loss=2.8071\n",
            "Epoch 297 | train_loss=4.4133 | val_loss=2.8083\n",
            "Epoch 298 | train_loss=3.8310 | val_loss=2.8052\n",
            "Epoch 299 | train_loss=3.9160 | val_loss=2.8059\n",
            "Epoch 300 | train_loss=3.8256 | val_loss=2.8155\n",
            "Epoch 301 | train_loss=3.7315 | val_loss=2.8183\n",
            "Epoch 302 | train_loss=2.7449 | val_loss=2.8076\n",
            "Epoch 303 | train_loss=3.4355 | val_loss=2.7887\n",
            "Epoch 304 | train_loss=3.5652 | val_loss=2.7528\n",
            "Epoch 305 | train_loss=3.8243 | val_loss=2.7267\n",
            "Epoch 306 | train_loss=4.5393 | val_loss=2.7107\n",
            "Epoch 307 | train_loss=3.3627 | val_loss=2.6930\n",
            "Epoch 308 | train_loss=3.0896 | val_loss=2.7022\n",
            "Epoch 309 | train_loss=4.2384 | val_loss=2.6974\n",
            "Epoch 310 | train_loss=4.2591 | val_loss=2.7134\n",
            "Epoch 311 | train_loss=3.9759 | val_loss=2.7342\n",
            "Epoch 312 | train_loss=3.7058 | val_loss=2.7440\n",
            "Epoch 313 | train_loss=3.9532 | val_loss=2.7454\n",
            "Epoch 314 | train_loss=3.9635 | val_loss=2.7583\n",
            "Epoch 315 | train_loss=3.5163 | val_loss=2.7779\n",
            "Epoch 316 | train_loss=4.4739 | val_loss=2.8079\n",
            "Epoch 317 | train_loss=3.1233 | val_loss=2.8394\n",
            "Epoch 318 | train_loss=4.2030 | val_loss=2.8545\n",
            "Epoch 319 | train_loss=3.1121 | val_loss=2.8668\n",
            "Epoch 320 | train_loss=4.2781 | val_loss=2.8714\n",
            "Epoch 321 | train_loss=3.8337 | val_loss=2.8849\n",
            "Epoch 322 | train_loss=3.5871 | val_loss=2.8712\n",
            "Epoch 323 | train_loss=4.0738 | val_loss=2.8900\n",
            "Epoch 324 | train_loss=3.1261 | val_loss=2.8829\n",
            "Epoch 325 | train_loss=4.1001 | val_loss=2.8684\n",
            "Epoch 326 | train_loss=3.8531 | val_loss=2.8870\n",
            "Epoch 327 | train_loss=4.1469 | val_loss=2.8929\n",
            "⏹ Early stopping triggered. Best val_loss = 2.6402\n",
            "Stopping at epoch 327\n",
            "MAE (Test): 2.9559872\n",
            "MSE (Test): 11.459347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/captum/log/dummy_log.py:39: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
            "               activations. The hooks and attributes will be removed\n",
            "            after the attribution is finished\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total connections (SNP-Methylation): 43\n",
            "Total connections (Methylation-Gene): 3\n",
            "Total connections (Gene-Pathway): 63\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"5a3bf6a7-7fa4-42d1-89e2-c1eab14b48c3\" class=\"plotly-graph-div\" style=\"height:1500px; width:2000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5a3bf6a7-7fa4-42d1-89e2-c1eab14b48c3\")) {                    Plotly.newPlot(                        \"5a3bf6a7-7fa4-42d1-89e2-c1eab14b48c3\",                        [{\"arrangement\":\"snap\",\"link\":{\"source\":[0,0,0,0,1,2,2,2,2,3,4,4,5,5,5,5,5,5,5,6,7,7,7,7,8,8,9,10,11,11,11,12,12,12,12,13,13,13,13,14,14,14,14,20,20,36,37,37,37,37,37,37,38,38,38,38,38,38,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39],\"target\":[15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,19,20,21,22,23,25,26,34,35,20,21,23,20,21,22,23,20,21,22,23,20,21,22,23,37,38,39,40,41,42,43,44,45,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96],\"value\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]},\"node\":{\"color\":[\"#2E91E5\",\"#E15F99\",\"#1CA71C\",\"#FB0D0D\",\"#DA16FF\",\"#222A2A\",\"#B68100\",\"#750D86\",\"#EB663B\",\"#511CFB\",\"#00A08B\",\"#FB00D1\",\"#FC0080\",\"#B2828D\",\"#6C7C32\",\"#778AAE\",\"#862A16\",\"#A777F1\",\"#620042\",\"#1616A7\",\"#DA60CA\",\"#6C4516\",\"#0D2A63\",\"#AF0038\",\"#2E91E5\",\"#E15F99\",\"#1CA71C\",\"#FB0D0D\",\"#DA16FF\",\"#222A2A\",\"#B68100\",\"#750D86\",\"#EB663B\",\"#511CFB\",\"#00A08B\",\"#FB00D1\",\"#FC0080\",\"#B2828D\",\"#6C7C32\",\"#778AAE\",\"#862A16\",\"#A777F1\",\"#620042\",\"#1616A7\",\"#DA60CA\",\"#6C4516\",\"#0D2A63\",\"#AF0038\",\"#2E91E5\",\"#E15F99\",\"#1CA71C\",\"#FB0D0D\",\"#DA16FF\",\"#222A2A\",\"#B68100\",\"#750D86\",\"#EB663B\",\"#511CFB\",\"#00A08B\",\"#FB00D1\",\"#FC0080\",\"#B2828D\",\"#6C7C32\",\"#778AAE\",\"#862A16\",\"#A777F1\",\"#620042\",\"#1616A7\",\"#DA60CA\",\"#6C4516\",\"#0D2A63\",\"#AF0038\",\"#2E91E5\",\"#E15F99\",\"#1CA71C\",\"#FB0D0D\",\"#DA16FF\",\"#222A2A\",\"#B68100\",\"#750D86\",\"#EB663B\",\"#511CFB\",\"#00A08B\",\"#FB00D1\",\"#FC0080\",\"#B2828D\",\"#6C7C32\",\"#778AAE\",\"#862A16\",\"#A777F1\",\"#620042\",\"#1616A7\",\"#DA60CA\",\"#6C4516\",\"#0D2A63\",\"#AF0038\",\"#2E91E5\"],\"label\":[\"12:122086941\",\"21:45830713\",\"rs429358\",\"rs148729689\",\"19:16412710\",\"6:31574685\",\"21:45830716\",\"rs483082\",\"19:16412713\",\"14:36030727\",\"7:4777185\",\"rs6857\",\"rs5117\",\"rs438811\",\"rs34954997\",\"cg18135647\",\"cg08626888\",\"cg24467291\",\"cg09813248\",\"cg08422803\",\"cg04026543\",\"cg07928695\",\"cg26235567\",\"cg08436089\",\"cg10929921\",\"cg19823452\",\"cg08065229\",\"cg00585846\",\"cg17380469\",\"cg01067809\",\"cg12585943\",\"cg21485120\",\"cg11173754\",\"cg24926370\",\"cg11655734\",\"cg12400556\",\"cg02707669\",\"11725445_x_at\",\"11760472_x_at\",\"11729310_a_at\",\"GO:0072594\",\"GO:0097193\",\"GO:0072331\",\"GO:0072332\",\"GO:0042770\",\"GO:0008630\",\"GO:1901987\",\"GO:0010639\",\"GO:0051656\",\"GO:0000819\",\"GO:0098813\",\"GO:0000070\",\"GO:0007059\",\"GO:0140014\",\"GO:0000280\",\"GO:0007051\",\"GO:0045786\",\"GO:0007093\",\"GO:0000075\",\"GO:1901991\",\"GO:0010948\",\"GO:1901990\",\"GO:0050000\",\"GO:0008608\",\"GO:0051310\",\"GO:0051303\",\"GO:0051315\",\"GO:0007080\",\"GO:0045132\",\"GO:0051294\",\"GO:0033045\",\"GO:0051983\",\"GO:0033044\",\"GO:0033047\",\"GO:0051985\",\"GO:0051306\",\"GO:0007091\",\"GO:0044784\",\"GO:0051304\",\"GO:0030071\",\"GO:1905818\",\"GO:0010965\",\"GO:1902099\",\"GO:0007094\",\"GO:0071173\",\"GO:0071174\",\"GO:0031577\",\"GO:0033046\",\"GO:0033048\",\"GO:0045841\",\"GO:2000816\",\"GO:1902100\",\"GO:1905819\",\"GO:0007088\",\"GO:0045839\",\"GO:2001251\",\"GO:0051784\"],\"line\":{\"color\":\"black\",\"width\":0.5},\"pad\":10,\"thickness\":20,\"x\":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.33,0.66,0.66,0.66,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99,0.99]},\"type\":\"sankey\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"font\":{\"size\":14},\"height\":1500,\"width\":2000},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5a3bf6a7-7fa4-42d1-89e2-c1eab14b48c3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}